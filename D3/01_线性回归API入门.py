""""
线性回归介绍(Linear Regression)
    概述/目的：用线性公式描述多个自变量（特征）与1个因变量（标签）之间的关系，对其关系进行建模，基于特征预测标签。
    线性回归属于：有监督学习（有明确的特征与标签数据），且标签是连续值（如房价、气温等）。

    分类：根据特征数量分为两类：
        1. 一元线性回归：1个特征 + 1个标签（如“房屋面积”预测“房价”）；
        2. 多元线性回归：多个特征 + 1个标签（如“房屋面积+楼层+地段”预测“房价”）。

    公式：
    1. 一元线性回归：
    数学形式为y = kx + b
    机器学习符号对应：k（数学斜率）→ 权重(Weight)，简称w（表示特征对标签的影响程度）；
    b（数学截距）→ 偏置(Bias)，简称b（表示特征之外的基准值）。
    因此，一元线性回归的机器学习表达式为y = wx + b

    2.多元线性回归：展开式（以3个特征为例）为y = w1x1 + w2x2 + w3x3 + … + wnxn + b；
    向量形式（更简洁的矩阵表示）为y = w^T · x + b，
    其中w^T是权重向量w的转置（如[w1, w2, w3]转置后为列向量），
    x是特征向量（如[x1, x2, x3]），·是向量点积（表示特征与对应权重的加权和）。

总结：线性回归的本质是通过调整权重w和偏置b，使线性模型尽可能准确地拟合特征与标签之间的关系，从而实现对未知标签的预测。

    误差 = 预测值-真实值
    损失函数(Loss Function,也叫成本函数，代价函数，目标函数，Cost Function)  l'(x)
        用于描述每个样本点 和 其预测值之间关系的, 让损失函数最小，就是让 误差和小，线性回归效率，评估就越高

    问题：如何让损失函数最小？
    答案：
        思路1：正规方程法
        思路2：梯度下降法

    损失函数分类：
        最小二乘：每个样本点误差的平方和
        MSE(Mean Square Error,均方误差)：每个样本点误差的平方和/样本个数
        RMSE(Root Mean Square Error,均方根误差):均方误差 开平方根
        MAE(Mean Absolute Error,均绝对误差):每个样本点误差的绝对值和/样本个数

    矩阵相关
        1范数 = 向量中各元素 绝对值 之和
        2范数 = 向量的模长，即：各个元素平方和，开平方根  x为向量 x转置x=||x||2²
        Lp范数 = ||x||p=(|x1|p次方+...)p分之一次方
"""

# 导包
from sklearn.linear_model import LinearRegression

# 案例：演示线性回归API入门.

# 1.准备数据
x_train = [[160], [166], [172], [174], [180]]  # 训练集的特征
y_train = [56.3, 60.6, 65.1, 68.5, 75]  # 训练集的标签
x_test = [[176]]  # 测试集的特征

# 2.数据的预处理 这里不需要
# 3.特征工程（特征提取，特征预处理），这里不需要

# 4.模型训练
# 4.1 创建模型对象
estimator = LinearRegression()
# 4.2具体的训练动作.
estimator.fit(x_train, y_train)

# 4.3 因为是线性回归模型，我们可以查看下：斜率(w,权重),截距(b,偏置)
print(f'权重:{estimator.coef_}')          # 权重:[0.92942177]
print(f'偏置:{estimator.intercept_}')     # 偏置:-93.27346938775517
# 5.模型预测

y_pre=estimator.predict(x_test)
print(f'预测值为:{y_pre}')                  #预测值为:[70.3047619]
# 6.模型评估
