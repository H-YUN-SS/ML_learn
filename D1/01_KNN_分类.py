""" KNN算法介绍(K Nearest Neighbors), K近邻算法
    原理
        基于欧氏距离(或者其它距离计算方式)计算测试集和每个训练集之间的距离，然后根据距离升序排列，找到最近的K个样本。
        基于K个样本投票，票数多的就作为最终预测结果→分类问题。
        基于K个样本计算平均值，作为最终预测结果→回归问题。

    实现思路:
        1. 分类问题
        适用于: 有特征，有标签，且标签是不连续的(离散的)
        2. 回归问题
        适用于: 有特征，有标签，且标签是连续的。

    KNN算法，分类问题思路如下:
        1. 计算测试集和每个训练的样本之间的距离。
        2. 基于距离进行升序排列。
        3. 找到最近的K个样本。
        4. K个样本进行投票。
        5. 票数多的结果，作为最终的预测结果。
"""
# 代码实现思路:
#         1. 导包。
from sklearn.neighbors import KNeighborsClassifier
#         2. 准备数据集(测试集 和 训练集)。
x_train=[[0],[1],[2],[3]]               #训练集的特征数据(特征可以有多个 所以必须写成二维)
y_train=[0,0,1,1]                       #训练集的标签数据(标签是离散的，所以为一维)
x_test=[[5]]                            #测试集的特征数据
#         3. 创建(KNN 分类模型)模型对象。
estimator=KNeighborsClassifier(n_neighbors=2)#评估器（模型对象）/model   k=2
#         4. 模型训练。传入训练集的特征数据
estimator.fit(x_train,y_train)#拟合
#         5. 模型预测。
y_pre=estimator.predict(x_test)
#         6. 打印预测结果。
print(f'预测值为:{y_pre}')
